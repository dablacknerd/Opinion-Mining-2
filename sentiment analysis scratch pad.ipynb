{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para = \"\"\"\n",
    "This cute little set is not only sturdy and realistic, it was also a wonderful introduction to\n",
    "preparing food for our 3 year old daughter. Ever since her Grandpa bought this for her, she's\n",
    "made everything from a cheese sandwhich to a triple decker salami club! She had so much fun\n",
    "playing with this toy, that she started to become interested in how I prepared meals. She now is\n",
    "very eager to spread peanut butter and jam, layer turkey and cheese and help mix cake batter.\n",
    "A very cute gift to give a girl or boy.\n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis cute little set is not only sturdy and realistic, it was also a wonderful introduction to\\npreparing food for our 3 year old daughter.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The best scene in the movie wa when Gerardo is trying to find a song that keep running through his head .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =\"The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.\"\n",
    "\n",
    "text_token = word_tokenize(text)\n",
    "text_pos = pos_tag(text_token)\n",
    "text_pos[0]\n",
    "text_pos_norm =[]\n",
    "new_text =[]\n",
    "for t in text_pos:\n",
    "    lem = lemma.lemmatize(t[0])\n",
    "    text_pos_norm.append((lem,t[1]))\n",
    "    new_text.append(lem)\n",
    "    \n",
    "' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "def create_syn_set_annotation(text_pos_normalized,text):\n",
    "    text_syn_set_list =[]\n",
    "    for t in text_pos_normalized:\n",
    "        if t[1][:2] =='NN':\n",
    "            if lesk(text,t[0],'n'):\n",
    "                text_syn_set_list.append(lesk(text,t[0],'n'))\n",
    "        elif t[1][:2] =='VB':\n",
    "            if lesk(text,t[0],'v'):\n",
    "                text_syn_set_list.append(lesk(text,t[0],'v'))\n",
    "        elif t[1][:2] =='RB':\n",
    "            if lesk(text,t[0],'r'):\n",
    "                text_syn_set_list.append(lesk(text,t[0],'r'))\n",
    "        elif t[1][:2] =='JJ':\n",
    "            if lesk(text,t[0],'a'):\n",
    "                text_syn_set_list.append(lesk(text,t[0],'a'))\n",
    "        else:\n",
    "            continue\n",
    "    return text_syn_set_list\n",
    "syn_list = create_syn_set_annotation(text_pos_norm,new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.a.01'),\n",
       " Synset('scenery.n.01'),\n",
       " Synset('movie.n.01'),\n",
       " Synset('be.v.12'),\n",
       " Synset('try_on.v.01'),\n",
       " Synset('find.v.13'),\n",
       " Synset('sung.n.01'),\n",
       " Synset('retain.v.02'),\n",
       " Synset('tend.v.01'),\n",
       " Synset('head.n.25')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def polarity_score(text_syn_set_list):\n",
    "    limp =[]\n",
    "    for s in text_syn_set_list:\n",
    "        pos_score = swn.senti_synset(s.name()).pos_score()\n",
    "        neg_score = swn.senti_synset(s.name()).neg_score()\n",
    "        neut_score = swn.senti_synset(s.name()).obj_score()\n",
    "        polarity_score = 0\n",
    "        if pos_score > neg_score and pos_score > neut_score:\n",
    "            polarity_score = 1\n",
    "        elif neg_score > pos_score and neg_score > neut_score:\n",
    "            polarity_score = -1\n",
    "        else:\n",
    "            polarity_score = 0\n",
    "        limp.append(polarity_score)\n",
    "    arr = np.array(limp)\n",
    "    return round(arr.mean(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_score(syn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    text_token = word_tokenize(text)\n",
    "    text_pos = pos_tag(text_token)\n",
    "    text_pos_norm =[]\n",
    "    new_text_list =[]\n",
    "    for t in text_pos:\n",
    "        lem = lemma.lemmatize(t[0])\n",
    "        text_pos_norm.append((lem,t[1]))\n",
    "        new_text_list.append(lem)\n",
    "    new_text = ' '.join(new_text_list)\n",
    "    syn_set_list = create_syn_set_annotation(text_pos_norm,new_text)\n",
    "    polarity = polarity_score(syn_set_list)\n",
    "    return polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis('A very, very, very slow-moving, aimless movie about a distressed, drifting young man.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis('Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subjectivity_score(text_syn_set_list):\n",
    "    limp = []\n",
    "    for s in text_syn_set_list:\n",
    "        limp.append(swn.senti_synset(s.name()).obj_score())\n",
    "    arr = np.array(limp)\n",
    "    return round(np.mean(limp),1)\n",
    "objectivity_score(syn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
